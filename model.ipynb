{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "import random\n",
    "from transformers import BartForConditionalGeneration, BartTokenizer, Seq2SeqTrainer, Seq2SeqTrainingArguments,DataCollatorForSeq2Seq\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import datasets\n",
    "import numpy as np\n",
    "import chardet\n",
    "import torch\n",
    "from onnx2keras import onnx_to_keras\n",
    "import onnx\n",
    "import onnxruntime as ort\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = \"./News\"\n",
    "articles_dir = os.path.join(base_dir, \"News Articles\")\n",
    "summaries_dir = os.path.join(base_dir, \"Summaries\")\n",
    "categories = [\"business\", \"entertainment\",\"politics\",\"sport\",\"tech\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_files(dir_path):\n",
    "    all_files = []\n",
    "    for category in categories:\n",
    "        category_dir = os.path.join(dir_path, category)\n",
    "        files = glob(os.path.join(category_dir,\"*.txt\"))\n",
    "        all_files.extend(files)\n",
    "    return all_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = load_files(articles_dir)\n",
    "summaries = load_files(summaries_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for article_file, summary_file in zip(articles, summaries):\n",
    "    with open(article_file, 'rb') as f:\n",
    "        result = chardet.detect(f.read())\n",
    "        encoding = result['encoding']\n",
    "    with open(article_file, 'r', encoding=encoding) as f:\n",
    "        article = f.read()\n",
    "    with open(summary_file, 'rb') as f:\n",
    "        result = chardet.detect(f.read())\n",
    "        encoding = result['encoding']\n",
    "    with open(summary_file, 'r', encoding=encoding) as f:\n",
    "        summary = f.read()\n",
    "    data.append((article, summary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(data)\n",
    "train_data = data[:int(0.8*len(data))]\n",
    "val_data = data[int(0.8*len(data)):int(0.9*len(data))]\n",
    "test_data = data[int(0.9*len(data)):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BartForConditionalGeneration.from_pretrained('facebook/bart-base')\n",
    "tokenizer = BartTokenizer.from_pretrained('facebook/bart-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text, tokenizer, max_length=1024):\n",
    "    encoded = tokenizer.encode_plus(\n",
    "        text,\n",
    "        max_length=max_length,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    input_ids = encoded['input_ids']\n",
    "    attention_mask = encoded['attention_mask']\n",
    "    return input_ids, attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data = []\n",
    "for article, summary in data:\n",
    "    input_ids, attention_mask = tokenize(article, tokenizer)\n",
    "    target_ids, _ = tokenize(summary, tokenizer, max_length=128)\n",
    "    processed_data.append({\n",
    "        'input_ids': input_ids.squeeze(0),\n",
    "        'attention_mask': attention_mask.squeeze(0),\n",
    "        'labels': target_ids.squeeze(0)\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 512\n",
    "def split_into_chunks(tensor, max_length=512, stride=256):\n",
    "    chunks = []\n",
    "    for i in range(0, tensor.size(0), stride):\n",
    "        chunk = tensor[i:i + max_length]\n",
    "        if chunk.size(0) < max_length:\n",
    "            padding = torch.full((max_length - chunk.size(0),), tokenizer.pad_token_id, dtype=torch.long)\n",
    "            chunk = torch.cat((chunk, padding), dim=0)\n",
    "        chunks.append(chunk)\n",
    "    return chunks\n",
    "\n",
    "new_processed_data = []\n",
    "\n",
    "for data_point in processed_data:\n",
    "    input_ids = data_point['input_ids']\n",
    "    attention_mask = data_point['attention_mask']\n",
    "    labels = data_point['labels']\n",
    "    if input_ids.size(0) > max_length:\n",
    "        input_chunks = split_into_chunks(input_ids)\n",
    "        mask_chunks = split_into_chunks(attention_mask)\n",
    "        for input_chunk, mask_chunk in zip(input_chunks, mask_chunks):\n",
    "            new_processed_data.append({\n",
    "                'input_ids': input_chunk,\n",
    "                'attention_mask': mask_chunk,\n",
    "                'labels': labels\n",
    "            })\n",
    "    else:\n",
    "        new_processed_data.append(data_point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SummarizationDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        return {\n",
    "            'input_ids': item['input_ids'],\n",
    "            'attention_mask': item['attention_mask'],\n",
    "            'labels': item['labels']\n",
    "        }\n",
    "    \n",
    "train_dataset = SummarizationDataset(new_processed_data[:int(0.8 * len(new_processed_data))])\n",
    "val_dataset = SummarizationDataset(new_processed_data[int(0.8 * len(new_processed_data)):int(0.9 * len(new_processed_data))])\n",
    "test_dataset = SummarizationDataset(new_processed_data[int(0.9 * len(new_processed_data)):])\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_16664\\800427156.py:1: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
      "  rouge = datasets.load_metric('rouge')\n",
      "c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\datasets\\load.py:759: FutureWarning: The repository for rouge contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.1/metrics/rouge/rouge.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "rouge = datasets.load_metric('rouge')\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    result = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "    return {\n",
    "        'rouge1': result['rouge1'].mid.fmeasure,\n",
    "        'rouge2': result['rouge2'].mid.fmeasure,\n",
    "        'rougeL': result['rougeL'].mid.fmeasure\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir='./results',\n",
    "    evaluation_strategy='epoch',\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=4,\n",
    "    predict_with_generate=True,\n",
    "    fp16=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset, \n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3560 [00:00<?, ?it/s]c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\data\\data_collator.py:646: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ..\\torch\\csrc\\utils\\tensor_new.cpp:277.)\n",
      "  batch[\"labels\"] = torch.tensor(batch[\"labels\"], dtype=torch.int64)\n",
      " 14%|█▍        | 500/3560 [2:24:39<14:55:20, 17.56s/it]Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.868, 'grad_norm': 9.16285228729248, 'learning_rate': 1.7191011235955056e-05, 'epoch': 0.56}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 890/3560 [4:22:34<13:51:50, 18.69s/it]c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\generation\\utils.py:1168: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "                                                       \n",
      " 25%|██▌       | 890/3560 [4:50:26<13:51:50, 18.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.2086851596832275, 'eval_rouge1': 0.14401848773563136, 'eval_rouge2': 0.07378554002845934, 'eval_rougeL': 0.12266539776124694, 'eval_runtime': 1672.5973, 'eval_samples_per_second': 0.532, 'eval_steps_per_second': 0.067, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 1000/3560 [5:22:52<12:25:01, 17.46s/it] Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.4436, 'grad_norm': 3.3941922187805176, 'learning_rate': 1.4382022471910113e-05, 'epoch': 1.12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 1500/3560 [7:48:58<9:56:25, 17.37s/it] Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.2269, 'grad_norm': 4.4554901123046875, 'learning_rate': 1.157303370786517e-05, 'epoch': 1.69}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 1780/3560 [9:10:49<8:38:09, 17.47s/it] c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\generation\\utils.py:1168: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "                                                       \n",
      " 50%|█████     | 1780/3560 [9:37:59<8:38:09, 17.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.155675172805786, 'eval_rouge1': 0.14926996047544439, 'eval_rouge2': 0.07834402299668625, 'eval_rougeL': 0.12769330226132597, 'eval_runtime': 1629.1499, 'eval_samples_per_second': 0.546, 'eval_steps_per_second': 0.069, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 2000/3560 [10:42:10<7:34:45, 17.49s/it]  Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.1339, 'grad_norm': 3.8989386558532715, 'learning_rate': 8.764044943820226e-06, 'epoch': 2.25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 2500/3560 [13:07:26<5:08:13, 17.45s/it]Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0291, 'grad_norm': 4.427305698394775, 'learning_rate': 5.955056179775281e-06, 'epoch': 2.81}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 2670/3560 [13:56:57<4:19:22, 17.49s/it]c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\generation\\utils.py:1168: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "                                                        \n",
      " 75%|███████▌  | 2670/3560 [14:23:42<4:19:22, 17.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.1418538093566895, 'eval_rouge1': 0.14090125847060544, 'eval_rouge2': 0.07781531360220086, 'eval_rougeL': 0.12319972060413245, 'eval_runtime': 1605.7846, 'eval_samples_per_second': 0.554, 'eval_steps_per_second': 0.07, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 3000/3560 [15:59:47<2:41:36, 17.32s/it]   Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0039, 'grad_norm': 3.84087872505188, 'learning_rate': 3.146067415730337e-06, 'epoch': 3.37}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 3500/3560 [18:24:43<17:11, 17.20s/it]  Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9729, 'grad_norm': 3.815361976623535, 'learning_rate': 3.3707865168539325e-07, 'epoch': 3.93}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3560/3560 [18:42:07<00:00, 17.46s/it]c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\generation\\utils.py:1168: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "                                                      \n",
      "100%|██████████| 3560/3560 [19:08:44<00:00, 19.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.134305953979492, 'eval_rouge1': 0.14758449181693845, 'eval_rouge2': 0.08042293515753574, 'eval_rougeL': 0.12762804765803415, 'eval_runtime': 1596.6955, 'eval_samples_per_second': 0.557, 'eval_steps_per_second': 0.07, 'epoch': 4.0}\n",
      "{'train_runtime': 68924.4628, 'train_samples_per_second': 0.413, 'train_steps_per_second': 0.052, 'train_loss': 2.2356609301620654, 'epoch': 4.0}\n",
      "Train output: TrainOutput(global_step=3560, training_loss=2.2356609301620654, metrics={'train_runtime': 68924.4628, 'train_samples_per_second': 0.413, 'train_steps_per_second': 0.052, 'total_flos': 8682647165337600.0, 'train_loss': 2.2356609301620654, 'epoch': 4.0})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_output = trainer.train()\n",
    "print(\"Train output:\", train_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n"
     ]
    }
   ],
   "source": [
    "model.save_pretrained('summary_model')\n",
    "tokenizer.save_pretrained(\"summary_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model zapisany w formacie ONNX do pliku: summary_model.onnx\n"
     ]
    }
   ],
   "source": [
    "sample_input_ids = torch.tensor([tokenizer.encode(\"Sample input text\", max_length=1024, padding=\"max_length\", truncation=True)])\n",
    "sample_attention_mask = torch.ones_like(sample_input_ids)\n",
    "onnx_model_path = \"summary_model.onnx\"\n",
    "torch.onnx.export(\n",
    "    model,\n",
    "    (sample_input_ids, sample_attention_mask),\n",
    "    onnx_model_path,\n",
    "    opset_version=14,\n",
    "    input_names=['input_ids', 'attention_mask'],\n",
    "    output_names=['output'],\n",
    "    dynamic_axes={'input_ids': {0: 'batch_size'}, 'attention_mask': {0: 'batch_size'}, 'output': {0: 'batch_size'}}\n",
    ")\n",
    "\n",
    "print(f\"Model zapisany w formacie ONNX do pliku: {onnx_model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\generation\\utils.py:1168: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "100%|██████████| 112/112 [27:58<00:00, 14.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval results: {'eval_loss': 2.134305953979492, 'eval_rouge1': 0.14758449181693845, 'eval_rouge2': 0.08042293515753574, 'eval_rougeL': 0.12762804765803415, 'eval_runtime': 1693.6747, 'eval_samples_per_second': 0.525, 'eval_steps_per_second': 0.066, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "eval_results = trainer.evaluate()\n",
    "print(\"Eval results:\", eval_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skrócony tekst:\n",
      "The firm, which is now one of the biggest investors in Google, benefited from sales of high-speed internet connections and higher advert sales.TimeWarner said fourth quarter sales rose 2% to $11.1bn from $10.9bn.Its profits were buoyed by one-off gains which offset a profit dip at Warner Bros, and less users for AOL.It lost 464,000 subscribers in the fourth quarter profits were lower than in the preceding three quarters.It hopes to increase subscribers by offering the online service free to TimeWarner internet customers and will try to sign up AOL's existing customers for high\n"
     ]
    }
   ],
   "source": [
    "def read_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "    return text\n",
    "file_path = \"./News/News Articles/business/001.txt\"\n",
    "text = read_file(file_path)\n",
    "inputs = tokenizer(text, return_tensors=\"pt\", max_length=1024, padding=\"max_length\", truncation=True)\n",
    "summary_ids = model.generate(inputs.input_ids, max_length=150, min_length=40, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
    "summary_text = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "print(\"Skrócony tekst:\")\n",
    "print(summary_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
